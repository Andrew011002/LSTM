{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tonimo/Documents/Learning-Developing-Software/Python/Machine Learning/Supervised Learning/Recurrent Neural Network\n"
     ]
    }
   ],
   "source": [
    "# For the Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# For the Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n",
    "\n",
    "# For saving\n",
    "from path import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data/wonderful_wizard_of_oz.txt', 'r').read() # (any file of text that can be converted to a string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k Baum\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with al\n"
     ]
    }
   ],
   "source": [
    "print(data[70:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'the', 'of', 'by', 'frank', 'is', 'for', 'the', 'use', 'of']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/tonimo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Before we begin to create and learn the model we have to clean the text. Similar to sentiment analysis, we will use we will have to rid \n",
    "non-words only. Although, we will need to keep stop words in the text as they are informative to what the next word will be. In order \n",
    "to process this text we will remove all symbols that are not letters, numbers, or spaces. Then we will have to split our text into tokens which\n",
    "are essentially smaller chunks of information rather than the entire sequence of text. Once we have our text in tokens, we will have to rid the \n",
    "tokens of non-words, punctuation, and anything outside of what is important to prediction the next word based on the words that proceed it.\n",
    "We can use some basic built in python functions, classes, etc. But we can also use some classes and external libraries such as nltk and a \n",
    "Tokenizar from keras.utils\"\"\"\n",
    "\n",
    "def preprocess(text):\n",
    "    nltk.download(\"words\") # (MAC OS ONLY)\n",
    "    words = set(nltk.corpus.words.words()) # words set\n",
    "    tokenized = text.split() # creating tokens\n",
    "    tokenized = [word.lower() for word in tokenized if word in words] # filtering tokens of words we don't want (not in words corpus)\n",
    "    return tokenized    \n",
    "\n",
    "print(preprocess(data)[:10]) # tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/tonimo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of the of by', 'the of by frank', 'of by frank is', 'by frank is for', 'frank is for the', 'is for the use', 'for the use of', 'the use of anyone', 'use of anyone anywhere', 'of anyone anywhere in']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"In order for our model to be able to predict the next word, we need to create sequences of words. We can design a desired length of the \n",
    "sequence which indicates how many words will be understood/processed before the next word is predicted. We will reserve tokens for the inputs \n",
    "and have one extra token (pad)* for the output (the next word) for a given sequence. Our interpretation will look something like this giving \n",
    "the following example: \"I went to play basketball today\" sequence length = 2. 1st iteration: [\"I went to\"] 2nd iteration: [\"went to play\"] 3rd \n",
    "iterations: [\"to play basketball\"]... and so. from 0 to our sequence length (exclusive) are our inputs and the last element in the array is the target or expected output. We will use these to embed information into vectors that will help us understand the positional relationships between these tokens (words).\"\"\"\n",
    "\n",
    "\n",
    "def sequenize(tokens, sequence_length, size=None):\n",
    "    length = sequence_length + 1 # include the output (+1)\n",
    "    sequences = []\n",
    "    size = size if size else len(tokens) # how many sequences you want\n",
    "\n",
    "    for i in range(length, size): \n",
    "        # getting the sequence along with the output for every possible sequence in our tokens\n",
    "        sequence = \" \".join(tokens[i-length:i]).strip()\n",
    "        # valid sequences only\n",
    "        if sequence and len(sequence.split()) == sequence_length + 1:\n",
    "            sequences.append(sequence) \n",
    "    \n",
    "    return sequences\n",
    "\n",
    "print(sequenize(preprocess(data), 3)[:10]) # sequences of our text with a sequence length of 3 + 1 (for output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"With the data in sequences we can see the inputs and outputs. The inputs are the first k (k = sequence length) words of the sequence and \n",
    "the output word comes after the kth word in the sequence. In order for our model to learn anything, we must use a class that turns these \n",
    "sequences into numerical vectors. The Keras API allows us to use the Tokenizer class which turns an array of sequences into a numerical vector, \n",
    "either turning the sequence into integers where the index represents the word at that position of the sequence and the element represents the key \n",
    "value of a key (token) in the dictionary of the tokenizer, or a vector of numbers where each number is binary for word count of tf-idf. What \n",
    "we're going to do is fit the Tokenizer to our sequences, to build a dictionary of words where the beginning keys of the dictionary will be \n",
    "tokens of hire frequency. Once we do that, we will then turn our text sequences into numerical sequences where the index represents where \n",
    "that word occurs in the sequence and the element is the key value of that tokens key in the tokenizer dictionary                                     \n",
    "\n",
    "Example: text = \"The earth is an awesome place live\" -> (tokenize) -> \n",
    "dictionary: {'e': 1, 'a': 2, 't': 3, 'h': 4, 'i': 5, 's': 6, 'l': 7, 'r': 8, 'n': 9, 'w': 10, 'o': 11, 'm': 12, 'p': 13, 'c': 14, 'v': 15}     \n",
    "test text = \"The earth is an great place live\" -> (text to sequence) -> \n",
    "sequence: [[3], [4], [1], [], [1], [2], [8], [3], [4], [], [5], [6], [], [2], [9], [], [], [8], [1], [2], [3], [], [13], [7], [2], [14], [1], [], [7], [5], [15], [1]] \n",
    "small note, the tokens are characters and not words in our case we have words so the keys will be words instead of characters\"\"\"\n",
    "\n",
    "def vectorize(sequences):\n",
    "    tokenizer = Tokenizer(oov_token=\"<OOV>\") # (oov_token is out of vocabulary token)\n",
    "    tokenizer.fit_on_texts(sequences) # fitting to create dictionary\n",
    "    vectors = np.array(tokenizer.texts_to_sequences(sequences)) # creating our numerical vectors for learning\n",
    "\n",
    "\n",
    "    return vectors, tokenizer # give back vectors array and the tokenizer for future predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/tonimo/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: ['of the of by frank is for the use of anyone', 'the of by frank is for the use of anyone anywhere', 'of by frank is for the use of anyone anywhere in', 'by frank is for the use of anyone anywhere in the', 'frank is for the use of anyone anywhere in the and']\n",
      "Vectors example: [  5   2   5  48 753  21  18   2 183   5 280]\n",
      "Total unique words: 2142\n",
      "Vocabulary size: 2143\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "\n",
    "text = sequenize(preprocess(data), sequence_length=sequence_length, size=50000) # 50,000 sequences of 10 words + 1 for output\n",
    "print(f\"sequences: {text[:5]}\")\n",
    "\n",
    "vectors, tokenizer = vectorize(text)\n",
    "count = len(tokenizer.index_word.keys()) # vocab of the tokenizer\n",
    "vocab_size = count + 1 # vocab size (for the embedding layer)\n",
    "print(f\"Vectors example: {vectors[0]}\\nTotal unique words: {count}\\nVocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors for each element in the input respectively:\n",
      "[[[ 0.03296374  0.03500713  0.03752087 -0.0340186   0.01835303]\n",
      "  [-0.04588386  0.01072135  0.00405924 -0.04999904  0.03682213]\n",
      "  [ 0.01428384  0.0389634  -0.00085402  0.00861896 -0.02453052]\n",
      "  [ 0.04725624  0.02515594  0.04757052 -0.01327684 -0.02088813]]]\n",
      "Weights for each vocab word in vocabulary (should be 10 (input dimension)):\n",
      "[<tf.Variable 'embedding/embeddings:0' shape=(10, 5) dtype=float32, numpy=\n",
      "array([[ 0.0298381 , -0.04962946,  0.03873776, -0.01433212,  0.04859641],\n",
      "       [ 0.03296374,  0.03500713,  0.03752087, -0.0340186 ,  0.01835303],\n",
      "       [-0.04588386,  0.01072135,  0.00405924, -0.04999904,  0.03682213],\n",
      "       [ 0.04516921, -0.01392338, -0.04011289, -0.01987324,  0.04825015],\n",
      "       [-0.00624734,  0.00655264,  0.02335049,  0.04804944,  0.04087234],\n",
      "       [ 0.01428384,  0.0389634 , -0.00085402,  0.00861896, -0.02453052],\n",
      "       [-0.00988655, -0.00684834,  0.02439785,  0.00412842,  0.02199061],\n",
      "       [ 0.04725624,  0.02515594,  0.04757052, -0.01327684, -0.02088813],\n",
      "       [ 0.00733464, -0.01197112, -0.02050892, -0.04362115,  0.04181748],\n",
      "       [ 0.01043347,  0.01516253,  0.01599337, -0.01609347, -0.03926501]],\n",
      "      dtype=float32)>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 21:05:06.988483: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Embedding is an important part of natural language processing and in this case creating a prediction on the next words. What an embedding layer\n",
    "does is give us the ability to learn our vectors, by assigning a \"look-up\" index which contains the weighted values assigned to that given word\n",
    "in it's position at the sequence. This is much more effeicient and better than a typical approach of assigning each word a vector and deploying\n",
    "it initially, because as the vocabulary size grows, the amount of computation & space becomes larger and thus causes training to be ineffecient.\n",
    "So assigning a vector or matrix to an embedded word is much better for processing. In a sense, it is a way of choosing features and reducing \n",
    "dimensionality. \n",
    "\n",
    "The Embedding layer from the Keras API has 3 important parameters. The input dimension, the output dimension, and the input length. \n",
    "The input dimension is the size of the vocabulary. Basically, these are our values being converted into one-hot encoding for however many \n",
    "words we have (hence it's the vocab size). The output dimensions is how big we want our vector of each word to be. We can start small and \n",
    "say something like 5, or we could large and bump it up to 500. This is basically a tunable parameter that helps with learning. Lastly, the \n",
    "input length is the maximum length of our sequences. If the sequences are not the same, it's wise to add pad (fill the sequence with zeros \n",
    "until it's the same length of the biggest sequence in the sequences). In our case, we're fixed with a length of ten so padding will not need\n",
    "to be added. From here, over epochs our model can learn the context of words, so that similar words have similar embeddings thus giving us \n",
    "predicted words alike after applying softmax (Theoretically)\"\"\"\n",
    "\n",
    "embedded_model = Sequential()\n",
    "embedded_model.add(Embedding(input_dim=10, output_dim=5, input_length=4))\n",
    "embedded_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=None)\n",
    "test_input = np.array([1, 2, 5, 7])\n",
    "test_input_vectors = embedded_model.predict(test_input.reshape(1, test_input.size))\n",
    "weights_for_vocab = embedded_model.weights\n",
    "print(f\"Vectors for each element in the input respectively:\\n{test_input_vectors}\")\n",
    "print(f\"Weights for each vocab word in vocabulary (should be 10 (input dimension)):\\n{weights_for_vocab}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM (Long Short Term Memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs -> lstm1 ->:\n",
      " [[[ 0.00497654 -0.11485896 -0.10108453]\n",
      "  [-0.06515366 -0.13458483 -0.23143965]]\n",
      "\n",
      " [[-0.05492815  0.06400284 -0.2400621 ]\n",
      "  [-0.05787418  0.06483869 -0.01606833]]\n",
      "\n",
      " [[-0.02756388  0.10773108  0.04353565]\n",
      "  [-0.09107605  0.12948447 -0.12584193]]\n",
      "\n",
      " [[ 0.0103193   0.06424125  0.06334271]\n",
      "  [-0.05525967  0.0982845  -0.11114873]]]\n",
      "\n",
      "output(lstm1) -> input -> lstm2 ->:\n",
      " [[ 0.04109912  0.02249972  0.05420007]\n",
      " [ 0.04302067  0.04966669  0.01984686]\n",
      " [ 0.03541845  0.0534464   0.00093455]\n",
      " [ 0.02065295  0.03397502 -0.00445839]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"The LSTM is a complex layer in a RNN which essentially helps us with learning during training by increasing effeciency and basically sifting \n",
    "the inputs of importance through the recurrent layers. An LSTM is a tatic that uses gates to control the flow of states as they recur through \n",
    "reuccrent layers. This is important to resolve issues with vanishing gradient from short-term memory of the the base recurrent neural network. \n",
    "\n",
    "To start off, the base LSTM has are 3 gates (1 with two operations (so 4 gates in a way)) that control the the flow of the information passed \n",
    "through. These gates basically help the cell learn what to keep and what to forget. The first gate is the forget gate which takes the previous \n",
    "cell state along with sigmoid applied to the concatenation of the previous hidden state and input vector. The sigmoid function squishes the \n",
    "values between 0 and 1 which makes important values closer to 1 and less important values closer to 0 which allows the network to retain \n",
    "important values and ignore (forget) non-important ones. The previous cell state is then combined with this output through multiplication. \n",
    "Next the input gate is used to update the cell state by applying sigmoid (to forget values or retain values) to the concatenation of the \n",
    "previous hidden state and the input vector and doing the same operation but instead of sigmoid, tanh is used to regulate the flow of the network. \n",
    "Those results are multiplied then added to the result of the forget gate to get the new cell state. Lastly, the output gate indicates what the \n",
    "next hidden state will be. It applies the sigmoid (to forget or retain values) to the concatenation of the input and previous hidden state then \n",
    "multiplies that with the result of the new cell state applied to the tanh function (to regulate the flow) to create the new hidden state to be \n",
    "passed to the next layer. In short, this process is complex, but it basically indicates to the network what values to keep or forget as it learns\n",
    "and back propagates. The LSTM outputs a new cell state and a previous hidden state to the next layer.\"\"\"\n",
    "\n",
    "inputs = tf.random.normal([4, 2, 2])\n",
    "lstm1 = LSTM(3, return_sequences=True)\n",
    "lstm2 = LSTM(3)\n",
    "output1 = lstm1(inputs) # lstm1 receiving inputs and passing them as sequential inputs (return_sequences=True)\n",
    "output2 = lstm2(output1) # lstm2 taking outputs from lstm1 as inputs to output data\n",
    "print(f\"inputs -> lstm1 ->:\\n {output1}\\n\")\n",
    "print(f\"output(lstm1) -> input -> lstm2 ->:\\n {output2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"First layer is an embedding layer that will learn the context of words based on their prediction which is a driving factor for predicting \n",
    "the next word. Using 32 as the vector size the represent a word (friendly dimension value)\n",
    "\n",
    "We then have two LSTM layers with 512 nodes for both. These layers implement a solution to vanishing gradient issues with RNN and is a huge \n",
    "factor for effectively and effeciently learning our features. Notice above that one sets the return sequences to True has it's False by default.\n",
    "This basically is what allows stacking of these layers because we want the next LSTM layer to recieve the output of the previous LSTM layer as \n",
    "time distributed input rather than one vector output as if there was nothing sequential. \n",
    "\n",
    "We then have a Dense layer (512 neurons) to apply the relu activation to the last time step of the previous LSTM layer (just a Hidden Layer \n",
    "before output). Stacked on top, we have a dropout layer to prevent overfitting.\n",
    "\n",
    "Lastly, we have another Dense layer as our output layer. The number of nodes needs to be the vocab size as we will later use argmax on the output vector to find which word the model predicted based on our vocab from the tokenizer. Of course for multi-class we must use softmax Activation\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=sequence_length))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(512))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30960, 11)\n",
      "Example input & shape: [  5   2   5  48 753  21  18   2 183   5], (10,)\n",
      "Example output & shape: 280, ()\n",
      "Goal: [  5   2   5  48 753  21  18   2 183   5] -> 280\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(vectors.shape)\n",
    "\n",
    "X, y = vectors[:, :-1], vectors[:, -1] # features are every word val up to last word, label is last word in a given vector\n",
    "\n",
    "print(f\"Example input & shape: {X[0]}, {X[0].shape}\") # vector of 3 values which are the indices to vocab\n",
    "print(f\"Example output & shape: {y[0]}, {y[0].shape}\") # vector of 1 value which is index to vocab\n",
    "print(f\"Goal: {X[0]} -> {y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "242/242 [==============================] - 50s 193ms/step - loss: 5.7152 - accuracy: 0.0983\n",
      "Epoch 2/25\n",
      "242/242 [==============================] - 42s 173ms/step - loss: 5.5464 - accuracy: 0.1021\n",
      "Epoch 3/25\n",
      "242/242 [==============================] - 44s 182ms/step - loss: 5.4752 - accuracy: 0.1021\n",
      "Epoch 4/25\n",
      "242/242 [==============================] - 47s 195ms/step - loss: 5.3946 - accuracy: 0.1047\n",
      "Epoch 5/25\n",
      "242/242 [==============================] - 46s 190ms/step - loss: 5.2743 - accuracy: 0.1057\n",
      "Epoch 6/25\n",
      "242/242 [==============================] - 46s 190ms/step - loss: 5.1544 - accuracy: 0.1080\n",
      "Epoch 7/25\n",
      "242/242 [==============================] - 46s 190ms/step - loss: 5.0082 - accuracy: 0.1199\n",
      "Epoch 8/25\n",
      "242/242 [==============================] - 46s 189ms/step - loss: 4.8283 - accuracy: 0.1408\n",
      "Epoch 9/25\n",
      "242/242 [==============================] - 45s 186ms/step - loss: 4.6532 - accuracy: 0.1547\n",
      "Epoch 10/25\n",
      "242/242 [==============================] - 45s 187ms/step - loss: 4.5035 - accuracy: 0.1641\n",
      "Epoch 11/25\n",
      "242/242 [==============================] - 45s 185ms/step - loss: 4.3744 - accuracy: 0.1703\n",
      "Epoch 12/25\n",
      "242/242 [==============================] - 45s 185ms/step - loss: 4.2374 - accuracy: 0.1791\n",
      "Epoch 13/25\n",
      "242/242 [==============================] - 45s 188ms/step - loss: 4.1091 - accuracy: 0.1865\n",
      "Epoch 14/25\n",
      "242/242 [==============================] - 46s 190ms/step - loss: 3.9719 - accuracy: 0.1954\n",
      "Epoch 15/25\n",
      "242/242 [==============================] - 47s 193ms/step - loss: 3.8074 - accuracy: 0.2057\n",
      "Epoch 16/25\n",
      "242/242 [==============================] - 45s 185ms/step - loss: 3.6066 - accuracy: 0.2239\n",
      "Epoch 17/25\n",
      "242/242 [==============================] - 44s 180ms/step - loss: 3.3674 - accuracy: 0.2464\n",
      "Epoch 18/25\n",
      "242/242 [==============================] - 44s 181ms/step - loss: 3.0795 - accuracy: 0.2832\n",
      "Epoch 19/25\n",
      "242/242 [==============================] - 44s 181ms/step - loss: 2.7424 - accuracy: 0.3391\n",
      "Epoch 20/25\n",
      "242/242 [==============================] - 46s 191ms/step - loss: 2.3492 - accuracy: 0.4106\n",
      "Epoch 21/25\n",
      "242/242 [==============================] - 44s 184ms/step - loss: 1.9636 - accuracy: 0.4933\n",
      "Epoch 22/25\n",
      "242/242 [==============================] - 43s 179ms/step - loss: 1.5716 - accuracy: 0.5851\n",
      "Epoch 23/25\n",
      "242/242 [==============================] - 43s 179ms/step - loss: 1.2476 - accuracy: 0.6664\n",
      "Epoch 24/25\n",
      "242/242 [==============================] - 45s 186ms/step - loss: 0.9487 - accuracy: 0.7433\n",
      "Epoch 25/25\n",
      "242/242 [==============================] - 45s 186ms/step - loss: 0.7234 - accuracy: 0.8061\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=25) # training (epochs vary for dataset and model build)\n",
    "model.save(f\"{path}/models/lstm.h5\") # saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just gets inputs for predicting next words\n",
    "def input_sequences(sequence_length):\n",
    "    print(\"Press enter to quit\")\n",
    "    sequences = []\n",
    "    while True:\n",
    "        user_input = input(f\"Enter a sequence of words to predict (any size): \").strip()\n",
    "        if user_input:\n",
    "            sequences.append(user_input)\n",
    "        else:\n",
    "            return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicts the next word off a given sequence\n",
    "def predict(tokenizer, inputs, model=None):\n",
    "    vectors = tokenizer.texts_to_sequences(inputs) # tokenize the inputs\n",
    "    vectors = np.asarray(pad_sequences(vectors, maxlen=sequence_length, padding=\"post\")) # pad the inputs\n",
    "    pred = model.predict(vectors) # predict the inputs\n",
    "    mappings = dict(map(reversed, tokenizer.word_index.items())) # reverse the tokenizer to get the word from the index\n",
    "    italic, end = \"\\033[3m\", \"\\033[0m\"\n",
    "    return [f\"{s}{italic} {mappings[np.argmax(p)]}{end}\" for s,p in zip(inputs, pred)] # generate what the model thinks the next word is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 10, 32)            68576     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 10, 512)           1116160   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 512)               2099200   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2143)              1099359   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,645,951\n",
      "Trainable params: 4,645,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(f\"{path}/models/lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press enter to quit\n",
      "When Aunt Em came there to live she was a\u001b[3m pretty\u001b[0m\n",
      "When Dorothy, who was an orphan, first came to her\u001b[3m great\u001b[0m\n",
      "with long silky hair and small black eyes that twinkled\u001b[3m running\u001b[0m\n",
      "He worked hard from morning till night and did not\u001b[3m know\u001b[0m\n",
      "The cyclone had set the house down very gently for\u001b[3m the\u001b[0m\n",
      "Dorothy carried the shoes into the house and placed them\u001b[3m they\u001b[0m\n",
      "She took a little basket and filled it with bread\u001b[3m from\u001b[0m\n",
      "some still open contradictions in my thoughts and my values, that\u001b[3m you\u001b[0m\n",
      "This automobile executive has a better idea than digging tunnels\u001b[3m all\u001b[0m\n",
      "We were very close and remain close. We just hadn’t\u001b[3m he\u001b[0m\n",
      "has no wires and total freedom. Explore new worlds with\u001b[3m the\u001b[0m\n",
      "wow that tv looks pretty nice\u001b[3m like\u001b[0m\n",
      "I went to play basketball and the\u001b[3m and\u001b[0m\n",
      "Are these the children of  Michael Jordan?\u001b[3m once\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "inputs = input_sequences(sequence_length) # get filtered sequences\n",
    "predictions = predict(tokenizer, inputs, model=model) # get predictions\n",
    "for pred in predictions:\n",
    "    print(pred) # italicized words are the prediction"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63847a374abcb3b9a1a98c0eeba015d87a63f2412b9d7a6f458ed5afc164c9c8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
